
**对于滑动窗口，我们需要多少内存来存储所有用户数据？** 让我们假设 'UserID' 占用 8 个字节。每个 epoch 时间将需要 4 个字节。假设我们需要每小时 500 个请求的速率限制。假设哈希表有 20 字节开销，Sorted Set 有 20 字节开销。最多，我们需要总共 12KB 来存储一个用户的数据：
`8 + (4 + 20 (sorted set overhead)) * 500 + 20 (hash-table overhead) = 12KB`

在这里，我们为每个元素保留 20 字节的开销。在一个排序集中，我们可以假设我们至少需要两个指针来维持元素之间的顺序——一个指针指向前一个元素，一个指向后一个元素。在 64 位机器上，每个指针将花费 8 个字节。所以我们需要 16 个字节用于指针。我们添加了一个额外的字（4 字节）来存储其他开销。

如果我们需要随时跟踪一百万用户，我们将需要的总内存为 12GB：
`12KB * 1 million ~= 12GB`

与固定窗口相比，滑动窗口算法占用大量内存；这将是一个可扩展性问题。如果我们能结合上述两种算法来优化我们的内存使用呢？

## 10. 带计数器的滑动窗口 (Sliding Window with Counters)

如果我们使用多个固定时间窗口（例如，我们速率限制时间窗口大小的 1/60）跟踪每个用户的请求计数会怎样。例如，如果我们有一个每小时的速率限制，我们可以每分钟保留一个计数，并在收到新请求时计算过去一小时内所有计数器的总和来计算节流限制。这将减少我们的内存占用。

让我们举个例子，我们的速率限制为每小时 500 个请求，每分钟还有 10 个请求的额外限制。这意味着当时间戳在过去一小时内的计数器总和超过请求阈值 (500) 时，Kristie 已经超过了速率限制。除此之外，她每分钟不能发送超过 10 个请求。这将是一个合理且实际的考虑因素，因为没有真正的用户会发送频繁的请求。即使他们这样做，他们也会在重试时看到成功，因为他们的限制每分钟都会重置。

我们可以将计数器存储在 Redis Hash 中，因为它为少于 100 个键提供了极其高效的存储。当每个请求增加哈希中的计数器时，它还会将哈希设置为一小时后过期。我们将每个“时间”标准化为一分钟。

[Diagram: Sliding Window with Counters]

**对于带计数器的滑动窗口，我们需要多少内存来存储所有用户数据？**
让我们假设 'UserID' 占用 8 个字节。每个 epoch 时间将需要 4 个字节，计数器将需要 2 个字节。假设我们需要每小时 500 个请求的速率限制。假设哈希表有 20 字节开销，Redis hash 有 20 字节。由于我们将每分钟保留一个计数，最多，我们需要为每个用户保留 60 个条目。我们需要总共 1.6KB 来存储一个用户的数据：
`8 + (4 + 2 + 20 (Redis hash overhead)) * 60 + 20 (hash-table overhead) = 1.6KB`

如果我们需要随时跟踪一百万用户，我们将需要的总内存为 1.6GB：
`1.6KB * 1 million ~= 1.6GB`

所以，我们的“带计数器的滑动窗口”算法使用的内存比简单的滑动窗口算法少 86%。

## 11. 数据分片和缓存

我们可以基于 'UserID' 进行分片以分发用户数据。为了容错和复制，我们应该使用一致性哈希。如果我们想为不同的 API 设置不同的节流限制，我们可以选择按每个用户的每个 API 进行分片。以 URL 缩短器为例；我们可以为每个用户或 IP 的 createURL() 和 deleteURL() API 设置不同的速率限制器。

如果我们的 API 是分区的，一个实际的考虑可能是为每个 API 分片也拥有一个单独的（稍小的）速率限制器。让我们以 URL 缩短器为例，我们要限制每个用户每小时创建不超过 100 个短 URL。假设我们对 createURL() API 使用基于哈希的分区，我们可以对每个分区进行速率限制，以允许用户每分钟创建不超过三个短 URL，以及每小时 100 个短 URL。

我们的系统可以从缓存最近活跃用户中获得巨大的好处。应用服务器在访问后端服务器之前可以快速检查缓存是否具有所需的记录。我们的速率限制器可以通过仅更新缓存中的所有计数器和时间戳来显著受益于**回写缓存 (Write-back cache)**。写入永久存储可以按固定间隔进行。这样我们可以确保速率限制器为用户请求增加的延迟最小。读取可以总是首先命中缓存；一旦用户达到最大限制并且速率限制器将只读取数据而没有任何更新，这将非常有用。

最近最少使用（LRU）对于我们的系统来说可能是一个合理的缓存逐出策略。

## 12. 我们应该按 IP 还是按用户进行速率限制？

让我们讨论使用这两种方案中每一种的优缺点：
*   **IP**：在这个方案中，我们按 IP 节流请求；虽然这在区分“好”和“坏”行为者方面不是最佳的，但仍然比根本没有速率限制要好。基于 IP 的节流的最大问题是当多个用户共享一个公共 IP 时，例如在网吧或使用相同网关的智能手机用户。一个坏用户可能会导致其他用户受到节流。另一个问题可能出现在缓存基于 IP 的限制时，因为甚至一台计算机也可以通过大量 IPv6 地址攻击，使服务器耗尽内存来跟踪 IPv6 地址是轻而易举的！
*   **User**：可以在用户身份验证后对 API 进行速率限制。一旦通过身份验证，将向用户提供一个令牌，用户将在每个请求中传递该令牌。这将确保我们将针对具有有效身份验证令牌的特定 API 进行速率限制。但是如果我们必须对登录 API 本身进行速率限制怎么办？这种速率限制的弱点在于，黑客可以通过输入错误凭据直到达到限制来对用户执行拒绝服务攻击；之后，实际用户将无法登录。

**如果我们结合上述两种方案会怎样？**

*   **Hybrid (混合)**：一种正确的方法是同时进行基于 IP 和基于用户的速率限制，因为它们单独实施时都有弱点，但这会导致每个条目包含更多细节的更多缓存条目，从而需要更多的内存和存储。

---

# 设计 Twitter Search (Twitter 搜索)

Twitter 是最大的社交网络服务之一，用户可以在其中分享照片、新闻和基于文本的消息。在本章中，我们将设计一个可以存储和搜索用户推文的服务。类似问题：Tweet search。难度等级：中等

## 1. 什么是 Twitter Search？

Twitter 用户可以随时更新他们的状态。每个状态（称为推文）由纯文本组成，我们的目标是设计一个允许搜索所有用户推文的系统。

## 2. 系统的需求和目标

*   让我们假设 Twitter 总共有 15 亿用户，其中 8 亿日活跃用户。
*   平均而言，Twitter 每天收到 4 亿条推文。
*   推文的平均大小为 300 字节。
*   让我们假设每天将有 5 亿次搜索。
*   搜索查询将由多个单词结合 AND/OR 组成。

我们需要设计一个可以有效地存储和查询推文的系统。

## 3. 容量估算和约束

**存储容量**：由于我们每天有 4 亿条新推文，每条推文平均 300 字节，我们需要的总存储空间将是：
`400M * 300 => 120GB/day`

每秒总存储：
`120GB / 24hours / 3600sec ~= 1.38MB/second`

## 4. 系统 API

我们可以使用 SOAP 或 REST API 来公开我们服务的功能；以下可以是搜索 API 的定义：

`search(api_dev_key, search_terms, maximum_results_to_return, sort, page_token)`

**参数**：
*   `api_dev_key` (string): 注册帐户的 API 开发者密钥。除此之外，这将用于根据分配的配额限制用户。
*   `search_terms` (string): 包含搜索词的字符串。
*   `maximum_results_to_return` (number): 要返回的推文数量。
*   `sort` (number): 可选排序模式：最新优先 (0 - 默认)，最佳匹配 (1)，最喜欢 (2)。
*   `page_token` (string): 此令牌将指定应返回的结果集中的页面。

**返回**：(JSON)
包含有关匹配搜索查询的推文列表信息的 JSON。每个结果条目可以包含用户 ID 和名称、推文文本、推文 ID、创建时间、点赞数等。

## 5. 高层设计

在高层面上，我们需要将所有状态存储在数据库中，并建立一个索引来跟踪哪个词出现在哪个推文中。这个索引将帮助我们快速找到用户试图搜索的推文。

## 6. 详细组件设计

**1. Storage (存储)**：我们每天需要存储 120GB 的新数据。鉴于这巨大的数据量，我们需要提出一个能够有效地将数据分发到多台服务器的数据分区方案。如果我们计划未来五年，我们将需要以下存储：
`120GB * 365days * 5years ~= 200TB`

如果我们希望任何时候都不超过 80% 的满载率，我们将大约需要 250TB 的总存储空间。让我们假设我们想要保留所有推文的额外副本以进行容错；那么，我们的总存储需求将是 500TB。如果我们假设一台现代服务器可以存储高达 4TB 的数据，我们将需要 125 台这样的服务器来保存未来五年所需的所有数据。

让我们从一个简单的设计开始，我们将推文存储在 MySQL 数据库中。我们可以假设我们将推文存储在一个具有两列 TweetID 和 TweetText 的表中。让我们假设我们基于 TweetID 对数据进行分区。如果我们的 TweetID 是系统范围内唯一的，我们可以定义一个哈希函数，将 TweetID 映射到存储服务器，我们可以在那里存储该推文对象。

**我们如何创建系统范围内唯一的 TweetID？** 如果我们每天收到 4 亿条新推文，那么我们在五年内可以预期多少推文对象？
`400M * 365 days * 5 years => 730 billion`

这意味着我们需要一个 5 字节的数字来唯一标识 TweetID。让我们假设我们有一个服务可以在我们需要存储对象时生成唯一的 TweetID（此处讨论的 TweetID 将类似于设计 Twitter 中讨论的 TweetID）。我们可以将 TweetID 馈送到我们的哈希函数以查找存储服务器并将我们的推文对象存储在那里。

**2. Index (索引)**：我们的索引应该是什么样子的？由于我们的推文查询将由单词组成，让我们建立一个可以告诉我们哪个词出现在哪个推文对象中的索引。让我们首先估算我们的索引有多大。如果我们想为所有英语单词和一些像人名、城市名等著名名词建立索引，并且如果我们假设我们有大约 30 万个英语单词和 20 万个名词，那么我们的索引中将有 50 万个单词。让我们假设单词的平均长度是五个字符。如果我们将索引保留在内存中，我们需要 2.5MB 的内存来存储所有单词：
`500K * 5 => 2.5 MB`

让我们假设我们只想将过去两年的所有推文的索引保留在内存中。由于我们在 5 年内将获得 7300 亿条推文，这将在两年内给我们 2920 亿条推文。鉴于每个 TweetID 将是 5 个字节，我们需要多少内存来存储所有 TweetID？
`292B * 5 => 1460 GB`

所以我们的索引就像一个巨大的分布式哈希表，其中“键”是单词，“值”是包含该单词的所有推文的 TweetID 列表。假设平均每条推文有 40 个单词，由于我们不会索引介词和其他小词如 'the', 'an', 'and' 等，让我们假设每条推文大约有 15 个单词需要索引。这意味着每个 TweetID 将在我们的索引中存储 15 次。所以我们需要总内存来存储我们的索引：
`(1460 * 15) + 2.5MB ~= 21 TB`

假设一台高端服务器有 144GB 内存，我们将需要 152 台这样的服务器来保存我们的索引。

我们可以基于两个标准对我们的数据进行分片：

**基于单词的分片**：在构建我们的索引时，我们将遍历推文的所有单词并计算每个单词的哈希以找到将对其进行索引的服务器。要查找包含特定单词的所有推文，我们只需查询包含此单词的服务器。
这种方法有几个问题：
1.  如果一个单词变热门了怎么办？那么持有该单词的服务器上会有大量查询。这种高负载将影响我们服务的性能。
2.  随着时间的推移，一些单词可能会比其他单词存储更多的 TweetID，因此，在推文增长时保持单词的均匀分布是相当棘手的。
要从这些情况中恢复，我们要么重新分区数据，要么使用一致性哈希。

**基于推文对象的分片**：在存储时，我们将 TweetID 传递给我们的哈希函数以找到服务器，并在该服务器上索引推文的所有单词。在查询特定单词时，我们必须查询所有服务器，每个服务器将返回一组 TweetID。中央服务器将聚合这些结果以将它们返回给用户。

## 7. 容错

**当索引服务器死机时会发生什么？** 我们可以为每台服务器拥有一个辅助副本，如果主服务器死机，它可以在故障转移后接管控制权。主服务器和辅助服务器将拥有相同的索引副本。

**如果主服务器和辅助服务器同时死机怎么办？** 我们必须分配一台新服务器并在其上重建相同的索引。我们如何做到这一点？我们不知道这台服务器上保存了什么单词/推文。如果我们使用“基于推文对象的分片”，蛮力解决方案是遍历整个数据库并使用我们的哈希函数过滤 TweetID，以找出存储在这台服务器上的所有所需推文。这将是低效的，并且在服务器重建期间，我们将无法从中服务任何查询，从而丢失一些本应由用户看到的推文。

**我们如何有效地检索推文和索引服务器之间的映射？** 我们必须建立一个反向索引，将所有 TweetID 映射到它们的索引服务器。我们的 Index-Builder 服务器可以保存此信息。我们需要建立一个哈希表，其中“键”是索引服务器编号，“值”是包含保存在该索引服务器上的所有 TweetID 的 HashSet。请注意，我们将所有 TweetID 保存在 HashSet 中；这将使我们能够快速地从我们的索引中添加/删除推文。所以现在，每当索引服务器必须重建自身时，它可以简单地询问 Index-Builder 服务器它需要存储的所有推文，然后获取这些推文以建立索引。这种方法肯定会很快。我们也应该拥有 Index-Builder 服务器的副本以进行容错。

## 8. 缓存

为了处理热门推文，我们可以在数据库前面引入缓存。我们可以使用 Memcached，它可以将所有此类热门推文存储在内存中。应用服务器在访问后端数据库之前，可以快速检查缓存是否具有该推文。根据客户端的使用模式，我们可以调整我们需要多少缓存服务器。对于缓存逐出策略，最近最少使用 (LRU) 似乎适合我们的系统。

## 9. 负载均衡

我们可以在系统的两个位置添加负载均衡层 1) 客户端和应用服务器之间和 2) 应用服务器和后端服务器之间。最初，可以采用简单的轮询方法；将传入请求平均分配给后端服务器。这个 LB 实现简单，不引入任何开销。这种方法的另一个好处是 LB 将把死服务器从轮换中取出，并停止向其发送任何流量。轮询 LB 的一个问题是它不会考虑服务器负载。如果服务器过载或缓慢，LB 不会停止向该服务器发送新请求。为了处理这个问题，可以放置一个更智能的 LB 解决方案，它定期查询后端服务器的负载并据此调整流量。

## 10. 排名

**如果我们想按社交图距离、流行度、相关性等对搜索结果进行排名怎么办？**
让我们假设我们想按流行度对推文进行排名，比如推文获得了多少点赞或评论等。在这种情况下，我们的排名算法可以计算一个“流行度数字”（基于点赞数等）并将其与索引一起存储。每个分区可以在将结果返回给聚合器服务器之前基于此流行度数字对结果进行排序。聚合器服务器合并所有这些结果，根据流行度数字对它们进行排序，并将热门结果发送给用户。
